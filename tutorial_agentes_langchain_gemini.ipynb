{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0509ae",
   "metadata": {},
   "source": [
    "# Agentes para Devs — Tutorial com LangChain + Gemini\n",
    "\n",
    "Este notebook cobre: preparação do ambiente em Python/VSCode, visão da arquitetura de agentes (runtimes, ferramentas, memória), introdução à LCEL (LangChain Expression Language) e um \"Hello, Agent!\" com Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f67023",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "1. Setup do ambiente (Python, virtualenv e VSCode)\n",
    "2. Visão geral da arquitetura de um agente\n",
    "3. Introdução ao LangChain e à LCEL\n",
    "4. Hello, Agent! (LLM + Parser + Prompt)\n",
    "5. Extensão: Memória de conversa\n",
    "6. Extensão: Ferramentas (Tools) com Gemini\n",
    "7. Próximos passos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23082c35",
   "metadata": {},
   "source": [
    "## 1. Setup do ambiente (Python, virtualenv e VSCode)\n",
    "\n",
    "1. **Instale Python 3.10+**: verifique com `python --version`.\n",
    "2. **Crie um virtualenv** no Windows:\n",
    "   - `python -m venv .venv`\n",
    "   - Ative: `\\.venv\\Scripts\\activate`\n",
    "\n",
    "   Em macOS/Linux:\n",
    "   - `python3 -m venv .venv`\n",
    "   - Ative: `source .venv/bin/activate`\n",
    "\n",
    "3. **VSCode**:\n",
    "   - Instale a extensão \"Jupyter\" (Microsoft).\n",
    "   - Abra este notebook e selecione o kernel do seu virtualenv.\n",
    "\n",
    "4. **Chave de API do Gemini**:\n",
    "   - Obtenha uma chave em: https://ai.google.dev/\n",
    "   - Defina `GOOGLE_API_KEY` no ambiente ou crie `.env` com `GOOGLE_API_KEY=<sua_chave>`.\n",
    "\n",
    "5. **No terminal**:\n",
    "   -  Instale todas as dependências necessárias\n",
    "   - `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f69f4d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY carregada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"GOOGLE_API_KEY\"), \"Defina GOOGLE_API_KEY no ambiente ou em um arquivo .env\"\n",
    "print(\"GOOGLE_API_KEY carregada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60511c85",
   "metadata": {},
   "source": [
    "## 2. Visão geral da arquitetura de um agente\n",
    "\n",
    "Agentes usam modelos de linguagem para decidir passos, chamar **ferramentas** e manter **memória**. Em LangChain, os blocos principais são:\n",
    "\n",
    "- **Runtimes/Executores (Runnables/Executors)**: definem o fluxo `prompt → llm → parser` e composições com ferramentas/memória.\n",
    "- **Ferramentas (Tools)**: funções que o agente pode invocar (cálculo, APIs, banco de dados).\n",
    "- **Memória (Memory)**: histórico de conversa/estado que influencia respostas futuras.\n",
    "\n",
    "Fluxo típico:\n",
    "1. Mensagens de entrada via `PromptTemplate`/`ChatPromptTemplate`.\n",
    "2. LLM (ex.: Gemini), opcionalmente com ferramentas.\n",
    "3. Parser (ex.: `StrOutputParser`) para normalizar a saída.\n",
    "4. Memória (opcional) para manter contexto.\n",
    "5. Executor/Agente que decide passos e uso de ferramentas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fa23a",
   "metadata": {},
   "source": [
    "## 3. Introdução ao LangChain e LCEL\n",
    "\n",
    "LCEL (LangChain Expression Language) compõe pipelines de forma declarativa. Exemplo: `prompt | llm | parser`. A seguir, configuramos o Gemini e executamos um pipeline simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7baf7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A capital de Portugal é Lisboa.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente conciso e direto.\"),\n",
    "    (\"human\", \"Responda: {pergunta}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "resposta = chain.invoke({\"pergunta\": \"Qual é a capital de Portugal?\"})\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd933bdf",
   "metadata": {},
   "source": [
    "## 4. Hello, Agent! (LLM + Parser + Prompt)\n",
    "\n",
    "Aqui integramos um prompt template, o LLM do Gemini e um parser de saída em um pipeline LCEL minimalista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b933d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá Agente! É um prazer conhecê-lo! Espero que você esteja tendo um dia fantástico!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "hello_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um agente amigável.\"),\n",
    "    (\"human\", \"Diga olá com entusiasmo para: {nome}\")\n",
    "])\n",
    "hello_chain = hello_prompt | llm | StrOutputParser()\n",
    "print(hello_chain.invoke({\"nome\": \"Agente\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e41b2",
   "metadata": {},
   "source": [
    "## 5. Extensão: Memória de conversa\n",
    "\n",
    "Usamos `RunnableWithMessageHistory` para manter histórico por sessão, permitindo respostas contextualizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9583c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interação 1:\n",
      "Olá! Como posso te ajudar hoje?\n",
      "\n",
      "Interação 2:\n",
      "Sua saudação foi \"Olá!\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "prompt_mem = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente útil e contextual.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "chain_mem = prompt_mem | llm | StrOutputParser()\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain_mem,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "print(\"Interação 1:\")\n",
    "out1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Olá!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(out1)\n",
    "\n",
    "print(\"Interação 2:\")\n",
    "out2 = chain_with_history.invoke(\n",
    "    {\"input\": \"Qual foi minha saudação?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78230adb",
   "metadata": {},
   "source": [
    "## 6. Extensão: Ferramentas (Tools) com Gemini\n",
    "\n",
    "Registramos uma função como ferramenta e criamos um \"agente que chama ferramentas\". O LLM escolhe quando invocar a ferramenta e com quais argumentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "206b3989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "{'name': 'somar', 'description': 'Soma dois números inteiros.\\n    Args:\\n        a: primeiro número\\n        b: segundo número\\n    Returns:\\n        Resultado da soma (int).', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `somar` with `{'a': 12.0, 'b': 30.0}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m42\u001b[0m{'name': 'somar', 'description': 'Soma dois números inteiros.\\n    Args:\\n        a: primeiro número\\n        b: segundo número\\n    Returns:\\n        Resultado da soma (int).', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}\n",
      "\u001b[32;1m\u001b[1;3mO resultado da soma de 12 e 30 é 42.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Resposta do agente: {'input': 'Some 12 e 30.', 'output': 'O resultado da soma de 12 e 30 é 42.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "@tool\n",
    "def somar(a: int, b: int) -> int:\n",
    "    \"\"\"Soma dois números inteiros.\n",
    "    Args:\n",
    "        a: primeiro número\n",
    "        b: segundo número\n",
    "    Returns:\n",
    "        Resultado da soma (int).\"\"\"\n",
    "    return a + b\n",
    "\n",
    "tools = [somar]\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um agente que usa ferramentas quando necessário.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") \n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "resultado = agent_executor.invoke({\n",
    "    \"input\": \"Some 12 e 30.\"\n",
    "})\n",
    "print(\"Resposta do agente:\", resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187a5e1",
   "metadata": {},
   "source": [
    "## 7. Próximos passos\n",
    "- Troque para `gemini-2.5-pro` quando precisar de respostas mais robustas.\n",
    "- Adicione ferramentas reais (busca web, banco de dados, APIs internas).\n",
    "- Persista memória (arquivo, Redis, base de dados) em vez de in-memory.\n",
    "- Crie testes unitários para ferramentas e fluxos.\n",
    "\n",
    "Bom proveito!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
