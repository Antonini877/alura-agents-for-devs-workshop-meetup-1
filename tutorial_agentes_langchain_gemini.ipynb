{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0509ae",
   "metadata": {},
   "source": [
    "# Agentes para Devs — Tutorial com LangChain + Gemini\n",
    "\n",
    "Este notebook cobre: preparação do ambiente em Python/VSCode, visão da arquitetura de agentes (runtimes, ferramentas, memória), introdução à LCEL (LangChain Expression Language) e um \"Hello, Agent!\" com Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f67023",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "1. Setup do ambiente (Python, virtualenv e VSCode)\n",
    "2. Visão geral da arquitetura de um agente\n",
    "3. Introdução ao LangChain e à LCEL\n",
    "4. Hello, Agent! (LLM + Parser + Prompt)\n",
    "5. Extensão: Memória de conversa\n",
    "6. Extensão: Ferramentas (Tools) com Gemini\n",
    "7. Próximos passos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23082c35",
   "metadata": {},
   "source": [
    "## 1. Setup do ambiente (Python, virtualenv e VSCode)\n",
    "\n",
    "1. **Instale Python 3.10+**: verifique com `python --version`.\n",
    "2. **Crie um virtualenv** no Windows:\n",
    "   - `python -m venv .venv`\n",
    "   - Ative: `\\.venv\\Scripts\\activate`\n",
    "\n",
    "   Em macOS/Linux:\n",
    "   - `python3 -m venv .venv`\n",
    "   - Ative: `source .venv/bin/activate`\n",
    "\n",
    "3. **VSCode**:\n",
    "   - Instale a extensão \"Jupyter\" (Microsoft).\n",
    "   - Abra este notebook e selecione o kernel do seu virtualenv.\n",
    "\n",
    "4. **Chave de API do Gemini**:\n",
    "   - Obtenha uma chave em: https://ai.google.dev/\n",
    "   - Defina `GOOGLE_API_KEY` no ambiente ou crie `.env` com `GOOGLE_API_KEY=<sua_chave>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação dos pacotes necessários\n",
    "# Execute esta célula dentro do seu virtualenv.\n",
    "!pip install -U langchain langchain-google-genai google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega a chave de API do Gemini a partir de variáveis de ambiente ou .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"GOOGLE_API_KEY\"), \"Defina GOOGLE_API_KEY no ambiente ou em um arquivo .env\"\n",
    "print(\"GOOGLE_API_KEY carregada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60511c85",
   "metadata": {},
   "source": [
    "## 2. Visão geral da arquitetura de um agente\n",
    "\n",
    "Agentes usam modelos de linguagem para decidir passos, chamar **ferramentas** e manter **memória**. Em LangChain, os blocos principais são:\n",
    "\n",
    "- **Runtimes/Executores (Runnables/Executors)**: definem o fluxo `prompt → llm → parser` e composições com ferramentas/memória.\n",
    "- **Ferramentas (Tools)**: funções que o agente pode invocar (cálculo, APIs, banco de dados).\n",
    "- **Memória (Memory)**: histórico de conversa/estado que influencia respostas futuras.\n",
    "\n",
    "Fluxo típico:\n",
    "1. Mensagens de entrada via `PromptTemplate`/`ChatPromptTemplate`.\n",
    "2. LLM (ex.: Gemini), opcionalmente com ferramentas.\n",
    "3. Parser (ex.: `StrOutputParser`) para normalizar a saída.\n",
    "4. Memória (opcional) para manter contexto.\n",
    "5. Executor/Agente que decide passos e uso de ferramentas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fa23a",
   "metadata": {},
   "source": [
    "## 3. Introdução ao LangChain e LCEL\n",
    "\n",
    "LCEL (LangChain Expression Language) compõe pipelines de forma declarativa. Exemplo: `prompt | llm | parser`. A seguir, configuramos o Gemini e executamos um pipeline simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Modelo Gemini (1.5-flash: rápido; 1.5-pro: maior qualidade)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente conciso e direto.\"),\n",
    "    (\"human\", \"Responda: {pergunta}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "resposta = chain.invoke({\"pergunta\": \"Qual é a capital de Portugal?\"})\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd933bdf",
   "metadata": {},
   "source": [
    "## 4. Hello, Agent! (LLM + Parser + Prompt)\n",
    "\n",
    "Aqui integramos um prompt template, o LLM do Gemini e um parser de saída em um pipeline LCEL minimalista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "hello_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um agente amigável.\"),\n",
    "    (\"human\", \"Diga olá com entusiasmo para: {nome}\")\n",
    "])\n",
    "hello_chain = hello_prompt | llm | StrOutputParser()\n",
    "print(hello_chain.invoke({\"nome\": \"Agente\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e41b2",
   "metadata": {},
   "source": [
    "## 5. Extensão: Memória de conversa\n",
    "\n",
    "Usamos `RunnableWithMessageHistory` para manter histórico por sessão, permitindo respostas contextualizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "prompt_mem = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente útil e contextual.\"),\n",
    "    (\"placeholder\", \"{history}\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "chain_mem = prompt_mem | llm | StrOutputParser()\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain_mem,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "print(\"Interação 1:\")\n",
    "out1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Olá!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(out1)\n",
    "\n",
    "print(\"\n",
    "Interação 2:\")\n",
    "out2 = chain_with_history.invoke(\n",
    "    {\"input\": \"Qual foi minha saudação?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78230adb",
   "metadata": {},
   "source": [
    "## 6. Extensão: Ferramentas (Tools) com Gemini\n",
    "\n",
    "Registramos uma função como ferramenta e criamos um \"agente que chama ferramentas\". O LLM escolhe quando invocar a ferramenta e com quais argumentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "@tool\n",
    "def somar(a: int, b: int) -> int:\n",
    "    \"\"\"Soma dois números inteiros.\n",
    "    Args:\n",
    "        a: primeiro número\n",
    "        b: segundo número\n",
    "    Returns:\n",
    "        Resultado da soma (int).\"\"\"\n",
    "    return a + b\n",
    "\n",
    "tools = [somar]\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um agente que usa ferramentas quando necessário.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "resultado = agent_executor.invoke({\n",
    "    \"input\": \"Some 12 e 30.\"\n",
    "})\n",
    "print(\"\n",
    "Resposta do agente:\", resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187a5e1",
   "metadata": {},
   "source": [
    "## 7. Próximos passos\n",
    "- Troque para `gemini-1.5-pro` quando precisar de respostas mais robustas.\n",
    "- Adicione ferramentas reais (busca web, banco de dados, APIs internas).\n",
    "- Persista memória (arquivo, Redis, base de dados) em vez de in-memory.\n",
    "- Crie testes unitários para ferramentas e fluxos.\n",
    "\n",
    "Bom proveito!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
